---
title: "19 Hypothesis 1"
author: "Timur Sharifullin"
date: "2023-04-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(ggeffects)
library(mgcv)
```


# Загрузка Данных
```{r}
final_df = read.csv("absolute_final_df.csv")
final_df$X = factor(final_df$X, levels = c("none","hit"))
final_df$artist_name = as.factor(final_df$artist_name)
final_df1 = final_df[,c("hit_n","X")]
```

# Hypothesis 1. Биография - предшествующих успех.

**The success of a given song is positively associated with the number of successful songs previously released by the artist, such that a higher number of previous successful songs in the artist's discography may increase the likelihood of the given song achieving chart success.**

Данная гипотеза может быть проверена при использовании логистической регрессии, так как гипотеза подразумевает линейное отношение между dependent и independent variable.

Единица анализа - трек 
Dependent Variable - X
Independent Variable - hit_n
Модель - Логистическая Регрессия

## Rule of Thumb

В данном случае в анализе участвует переменная, имеющая два класса, и только одна независимая переменная. Это значит, что соответствуясь Rule of Thumb для регрессионных моделей, должно быть минимум 10 обзерваций из каждого класса

```{r}
barn = final_df1 %>% group_by(hit_n,X) %>% summarize(n = n())
barn

barn %>%
  ggplot(aes(x = hit_n, y = n, fill = X)) +
  geom_bar(position="stack", stat="identity") +
  scale_x_continuous(breaks = seq(0,30,1)) +
  scale_y_continuous(breaks = seq(0,200,10)) +
  coord_cartesian(xlim = c(0,30), ylim = c(0,170)) +
  labs(x = "Number of Previous Hits", y = "Number of Apperances") +
  guides(fill=guide_legend(title="Success"))
```


## Models

```{r}
# logit 

m_logit = glm(data = filter(final_df1,hit_n<28), formula = X ~ hit_n,
              family = binomial(link = "logit"))


summary(m_logit)
#library(sjPlot)
#tab_model(m_logit)
```

Assumptions:
1. The outcome is a binary or dichotomous variable like yes vs no, positive vs negative, 1 vs 0.
TRUE


## Linearity Assumption

2.There is a linear relationship between the logit of the outcome and each predictor variables.

```{r}
# Predict the probability (p) of song success
probabilities <- predict(m_logit, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")

# Select only numeric predictors
mydata <- filter(final_df1,hit_n<28) %>% dplyr::select(hit_n)
predictors <- colnames(mydata)
# Bind the logit and tidying the data for plot

mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() #+ 
  #facet_wrap(~predictors, scales = "free_y")
```

TRUE

## Influential Values
- There is no influential values (extreme values or outliers) in the continuous predictors

```{r}
library(car)
influencePlot(m_logit, col = "red")
```

```{r}
m_logitfix <- update(m_logit, subset = c(-1,-212,-1637,-1638,-3642))
summary(m_logitfix)
```

- There is no high intercorrelations (i.e. multicollinearity) among the predictors.

There is only one IV.


## GAM

```{r}
m_logit1 = gam(data = filter(final_df1,hit_n<28), formula = X ~ s(hit_n),
              family = binomial(link = "logit"),subset = c(-1,-212,-1637,-1638,-3642))

summary(m_logit1)
```

```{r}
gam.check(m_logit1)
```

```{r}
anova(m_logit1,m_logitfix)
```

```{r}
# Splitting the data
ds3 = final_df1[c(-1,-212,-1637,-1638,-3642),] %>% filter(hit_n<28)
set.seed(1)

# using 70% of ds as training set and 30% as test set
sample = sample(c(TRUE, FALSE), nrow(ds3), replace=TRUE, prob=c(0.7,0.3))
train  = ds3[sample, ]
test   = ds3[!sample, ]

# Creating ROC Curve Plot

# GLM

m_logit_glm = glm(data = train, formula = X ~ hit_n,
              family = binomial(link = "logit"))

test_prob_glm = predict(m_logit_glm, newdata = test, type = "response")

# GAM

m_logit_gam = gam(data = train, formula = X ~ s(hit_n),
              family = binomial(link = "logit"))
test_prob_gam = predict(m_logit_gam, newdata = test, type = "response")


library(pROC)
test_roc_glm = roc(test$X ~ test_prob_glm, plot = TRUE, print.auc = TRUE)
test_roc_gam = roc(test$X ~ test_prob_gam, plot = TRUE, print.auc = TRUE)

print(paste("GLM Area Under Curve:",test_roc_glm$auc))
print(paste("GAM Area Under Curve:",test_roc_gam$auc))
```




```{r}
mydf <- ggpredict(m_logit1,terms = "hit_n")
mydf$model = "GAM Predictions"
mydf1 <- ggpredict(m_logitfix,terms = "hit_n")
mydf1$model = "GLM Predictions"

mydf = rbind(mydf,mydf1)


ggplot(mydf, aes(x, predicted,col=model)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.1) +
  scale_x_continuous(breaks = seq(0,28,1)) +
  scale_y_continuous(breaks = seq(0,1,0.1)) +
  labs(y = "Predicted Probability Of Hit", x = "Number of Previous Hits")
```



