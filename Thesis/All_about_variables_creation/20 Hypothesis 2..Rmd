---
title: "20 Hypothesis 2"
author: "Timur Sharifullin"
date: "2023-04-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(randomForest)
library(pdp)
library(ggpubr)
library(caret) # for model-building
library(DMwR) # for smote implementation
library(purrr) # for functional programming (map)
library(pROC) # for AUC calculations
library(pdp)
```

# Загрузка Данных
```{r}
final_df = read.csv("fin_df_distance_months.csv")
final_df$X = factor(final_df$X, levels = c("none","hit"))
final_df$artist_name = as.factor(final_df$artist_name) 
final_df1 = final_df[,c(3, 5, 18, 21:30)]
```
```{r}
library(mlbench)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(x=ds0[,2:11], y=ds0[,1], sizes=c(1:10), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```




# Hypothesis 2. Предельность похожести.

**As the degree of sound similarity between songs increases, their popularity will decrease up to a certain point due to the audience's need for uniqueness and differentiation from competitors. However, if the degree of sound similarity exceeds a certain point, this will lead to a decrease in popularity due to the loss of recognizability of the songs.**


Единица анализа - трек 
Dependent Variable - X
Independent Variable - 10 audio characteristics
Модель - GAM

## Rule of Thumb

В данном случае в анализе участвует переменная, имеющая два класса, и 10 независимых переменных. Это значит, что соответствуясь Rule of Thumb для регрессионных моделей, должно быть минимум 100 обзерваций из каждого класса

```{r}
barn = final_df1[2:13] %>% group_by(hit_n,X) %>% summarize(n = n())
barn

barn %>%
  ggplot(aes(x = hit_n, y = n, fill = X)) +
  geom_bar(position="stack", stat="identity") +
  scale_x_continuous(breaks = seq(0,300,10)) +
  scale_y_continuous(breaks = seq(0,4000,100)) +
  coord_cartesian(xlim = c(0,20), ylim = c(0,1000))
```
```{r}
barn
```

10 предикторов - по 100 обзерваций в каждой группе

Подходят первые шесть статусов.

## Разделение на subsets
```{r}
ds0 = final_df1 %>% filter(hit_n == 0) %>% select(-hit_n, -artist_name,-quiet_and_melodic)
ds1 = final_df1 %>% filter(hit_n == 1) %>% select(-hit_n, -artist_name)
ds2 = final_df1 %>% filter(hit_n == 2) %>% select(-hit_n, -artist_name)
ds3 = final_df1 %>% filter(hit_n == 3) %>% select(-hit_n, -artist_name)
ds4 = final_df1 %>% filter(hit_n == 4) %>% select(-hit_n, -artist_name)
ds5 = final_df1 %>% filter(hit_n == 5) %>% select(-hit_n, -artist_name)
```

## Models

1. Imbalance Data set
2. Random forest is affected by multicollinearity but not by outlier problem.
3. Impute missing values within random forest as proximity matrix as a measure

```{r}
data_to = ds0
```

```{r}
set.seed(2023)

sample = createDataPartition(data_to$X, p = 0.7, list = FALSE)
train  = data_to[sample, ]
test   = data_to[-sample, ]

#------------------------------------------------------------
# Set up control function for training

ctrl <- trainControl(
  method = "cv",
  classProbs = TRUE,
  number = 10)

smote_fit <- train(X ~ .,
                   data = train,
                   method = 'rf',
                   verbose = FALSE,
                   trControl = ctrl,
                   positive = "hit")

# Build a standard classifier using a gradient boosted machine

orig_fit <- train(X ~ .,
                  data = train,
                  method = 'rf',
                  trControl = ctrl,
                  positive = "hit")


# Build custom AUC function to extract AUC
# from the caret model object

test_roc <- function(model, data) {
  
  roc(data$X,
      predict(model, data, type = "prob")[, "hit"])

}


#------------------------------------------------------------

# Create model weights

model_weights <- ifelse(train$X == "none",
                        (1/table(train$X)[1]) * as.numeric(1/table(train$X)[2])/as.numeric(1/table(train$X)[1]),
                        (1/table(train$X)[2]) * 1)

# Use the same seed to ensure same cross-validation splits

ctrl$seeds <- orig_fit$control$seeds

# Build weighted model

weighted_fit <- train(X ~ .,
                      data = train,
                      method = 'rf',
                      weights = model_weights,
                      trControl = ctrl,
                      positive = "hit")

# Build down-sampled model

ctrl$sampling <- "down"

down_fit <- train(X ~ .,
                  data = train,
                  method = 'rf',
                  verbose = FALSE,
                  trControl = ctrl,
                  positive = "hit")

# Build up-sampled model

ctrl$sampling <- "up"

up_fit <- train(X ~ .,
                data = train,
                method = 'rf',
                verbose = FALSE,
                trControl = ctrl,
                positive = "hit")

# Build smote model

ctrl$sampling <- "smote"

smote_fit <- train(X ~ .,
                   data = train,
                   method = 'rf',
                   verbose = FALSE,
                   trControl = ctrl,
                   positive = "hit")
?train

#---------------------------------------------------------------------------

model_list <- list(original = orig_fit,
                   weighted = weighted_fit,
                   down = down_fit,
                   up = up_fit,
                   SMOTE = smote_fit)

model_list_roc <- model_list %>%
  map(test_roc, data = test)

auc_list = model_list_roc %>%
  map(auc) %>%
  as.data.frame() %>% t() %>% as.data.frame() %>%
  arrange(-V1)

#--------------------------------------------------------------------------------
rf_final <- switch(row.names(auc_list)[1],
  "SMOTE" = smote_fit,
  "up" = up_fit,
  "down" = down_fit,
  "original" = orig_fit,
  "weighted" = weighted_fit
)

# VIMP

varImp(rf_final)

# Confusion Matrix

cm = confusionMatrix(predict(orig_fit, test), test$X, positive = 'hit')
cm

# ROC Plot

test_pred = as.numeric(predict(rf_final, test))
roc.test <- roc(test$X ~ test_pred, plot = TRUE, print.auc = TRUE)
auc(roc.test)

#--------------------------------------------------------------------------------

print(paste("Balance Strategy:",row.names(auc_list)[1]))

#--------------------------------------------------------------------------------






rf_final$finalModel

train_pred = as.numeric(predict(rf_final, newdata=train))
roc.train <- roc(train$X ~ train_pred, plot = TRUE, print.auc = TRUE)
auc(roc.train)
?roc

```

```{r}
# Partial Dependence Plot

pd_list = list() 
for(i in seq(2,11)){
  pd = pdp::partial(pred.var = colnames(train)[i],
               object = rf_final,
               plot = F,
               prob = TRUE,
               which.class = "hit",
               grid.resolution = 40,
               progress = TRUE)
  pd_list[[i-1]] = pd
}


pd1 = autoplot(pd_list[[1]],ylab = "Probability of Hit")
pd2 = autoplot(pd_list[[2]],ylab = "Probability of Hit")
pd3 = autoplot(pd_list[[3]],ylab = "Probability of Hit")
pd4 = autoplot(pd_list[[4]],ylab = "Probability of Hit")
pd5 = autoplot(pd_list[[5]],ylab = "Probability of Hit")
pd6 = autoplot(pd_list[[6]],ylab = "Probability of Hit")
pd7 = autoplot(pd_list[[7]],ylab = "Probability of Hit")
pd8 = autoplot(pd_list[[8]],ylab = "Probability of Hit")
pd9 = autoplot(pd_list[[9]],ylab = "Probability of Hit")
pd10 = autoplot(pd_list[[10]],ylab = "Probability of Hit")

arrange_plot = ggarrange(pd1, pd2, pd3, pd4, pd5,
                        pd6, pd7, pd8, pd9, pd10,
                        ncol = 5, nrow = 2)
arrange_plot
```


```{r}

```

```{r}
# Partial Dependence Plot

pd_list = list() 
for(i in seq(2,11)){
  pd = pdp::partial(pred.var = colnames(train)[i],
               object = rf_final,
               plot = F,
               prob = TRUE,
               which.class = "hit",
               grid.resolution = 40,
               progress = TRUE)
  pd_list[[i-1]] = pd
}


pd1 = autoplot(pd_list[[1]],ylab = "Probability of Hit")
pd2 = autoplot(pd_list[[2]],ylab = "Probability of Hit")
pd3 = autoplot(pd_list[[3]],ylab = "Probability of Hit")
pd4 = autoplot(pd_list[[4]],ylab = "Probability of Hit")
pd5 = autoplot(pd_list[[5]],ylab = "Probability of Hit")
pd6 = autoplot(pd_list[[6]],ylab = "Probability of Hit")
pd7 = autoplot(pd_list[[7]],ylab = "Probability of Hit")
pd8 = autoplot(pd_list[[8]],ylab = "Probability of Hit")
pd9 = autoplot(pd_list[[9]],ylab = "Probability of Hit")
pd10 = autoplot(pd_list[[10]],ylab = "Probability of Hit")

pd_list[[1]]
ggarrange(pd1, pd2, pd3, pd4, pd5,
          pd6, pd7, pd8, pd9, pd10,
          ncol = 5, nrow = 2)
```

```{r}
rf_final_down %>%
  partial(pred.var = "rmsP_std",plot=T,which.class = "hit")
```


